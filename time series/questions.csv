,question,answers,,A,B,C,D,E,pictures,,feedback
1,Which of the following is an example of time series problem?,D,,Estimating number of hotel rooms booking in next 6 months.,Estimating the total sales in next 3 years of an insurance company.,Estimating the number of calls for the next one week.,All of the above,None of the above,,,All the above options have a time component associated.
2,Which of the following is not an example of a time series model?,D,,Kernel Smoothing,Lowess (Locally-weighted Scatterplot Smoothing),Moving Average,None of the above,,,,"Naive approach: Estimating technique in which the last period's actuals are used as this period's forecast, without adjusting them or attempting to establish causal factors. It is used only for comparison with the forecasts generated by the better (sophisticated) techniques.

In exponential smoothing, older data is given progressively-less relative importance whereas newer data is given progressively-greater importance.

In time series analysis, the moving-average (MA) model is a common approach for modeling univariate time series. The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term."
3,Which of the following can't be a component for a time series plot?,E,,Seasonality,Trend,Cyclical,Noise,None of the above,,,"A seasonal pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week). Seasonality is always of a fixed and known period. Hence, seasonal time series are sometimes called periodic time series

Seasonality is always of a fixed and known period. A cyclic pattern exists when data exhibit rises and falls that are not of fixed period.

Trend is defined as the 'long term' movement in a time series without calendar related and irregular effects, and is a reflection of the underlying level. It is the result of influences such as population growth, price inflation and general economic changes. The following graph depicts a series in which there is an obvious upward trend over time.

Noise: In discrete time, white noise is a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance.

Thus all of the above mentioned are components of a time series."
4,Which of the following is relatively easier to estimate in time series modeling?,A,,Seasonality,Cyclical,No difference between Seasonality and Cyclical,,,,,Seasonality exhibits fixed structure; it is easier to estimate.
5,The below time series plot contains both Cyclical and Seasonality component.,B,,TRUE,FALSE,,,,5.PNG,,There is a repeated trend in the plot above at regular intervals of time and is thus only seasonal in nature.
6,Adjacent observations in time series data (excluding white noise) are independent and identically distributed (IID).,B,,TRUE,FALSE,,,,,,Clusters of observations are frequently correlated with increasing strength as the time intervals between them become shorter. This needs to be true because in time series forecasting is done based on previous observations and not the currently observed data unlike classification or regression.
7,Smoothing parameter close to one gives more weight or influence to recent observations over the forecast.,A,,TRUE,FALSE,,,,,,"It may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past --- the smallest weights are associated with the oldest observations:
$$y_{T+1|T} = \alpha y_T + \alpha (1-\alpha) y_{T-1} + \alpha (1- \alpha )^2 y_{T-2} + ...$$
where \(0 \leq \alpha \leq 1\)  is the smoothing parameter. The one-step-ahead forecast for time \(T+1\) is a weighted average of all the observations in the series \(y_1,...,y_T\). The rate at which the weights decrease is controlled by the parameter  \(\alpha\)."
8,Sum of weights in exponential smoothing is _____.,B,,<1,1,>1,None of the above,,,,The sum of the weights even for a small  \(\alpha\) will be approximately one for any reasonable sample size.
9,The last period's forecast was 70 and demand was 60. What is the correct formula for simple exponential smoothing forecast with alpha of 0.4 for the next period.,D,,0.4*60 + 0.4*70 = 52,(1-0.4)*(60+70) = 78,0.4*70 + 0.6*60 = 64,0.4*60 + 0.6*70 = 66,,,,"\(Y_{t-1}= 70  S_{t-1}= 60  \alpha = 0.4  \)
Substituting the values we get: 0.4*60 + 0.6*70= 24 + 42= 66"
10,What does autocovariance measure?,D,,Linear dependence between multiple points on the different series observed at different times,Quadratic dependence between two points on the same series observed at different times,Linear dependence between two points on different series observed at same time,Linear dependence between two points on the same series observed at different times,,,,Option D is the definition of autocovariance.
11,Which of the following is not a necessary condition for weakly stationary time series?,D,,Mean is constant and does not depend on time,Autocovariance function depends on s and t only through their difference |s-t| (where t and s are moments in time),The time series under considerations is a finite variance process,Time series is Gaussian,,,,A Gaussian time series implies stationarity is strict stationarity.
12,Which of the following is not a technique used in smoothing time series? ,C,,Nearest Neighbour Regression,Locally weighted scatter plot smoothing,Tree based models like (CART),Smoothing Splines,,,,Time series smoothing and filtering can be expressed in terms of local regression models. Polynomials and regression splines also provide important techniques for smoothing. CART based models do not provide an equation to superimpose on time series and thus cannot be used for smoothing. All the other techniques are well documented smoothing techniques.
13,"If the demand is 100 during October 2016, 200 in November 2016, 300 in December 2016, 400 in January 2017. What is the 3-month simple moving average for February 2017?",A,,(200+300+400)/ 3 = 300,(100+200+300)/3 = 200,(200+300+400)/2 = 450,Need more information,,,,\(X`= (x_{t-3} + x_{t-2} + x_{t-1} ) /3 = (200+300+400)/ 3 = 900/3 =300\) 
14,"Looking at the below ACF plot, would you suggest to apply AR or MA in ARIMA modeling technique?",A,,AR,MA,Can't say,,,14.PNG,,"MA model is considered in the following situation, If the autocorrelation function (ACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative --- i.e., if the series appears slightly ""overdifferenced"" --- then consider adding an MA term to the model. The lag beyond which the ACF cuts off is the indicated number of MA terms. But as there are no observable sharp cutoffs the AR model must be preffered."
15,"Suppose, you are a data scientist at a company. And you observed the views on the articles increases during the month of Jan-Mar. Whereas the views during Nov-Dec decreases.

Does the above statement represent seasonality?",A,,TRUE,FALSE,Can't say,,,,,"Yes this is a definite seasonal trend as there is a change in the views at particular times. Remember, Seasonality is a presence of variations at specific periodic intervals."
16,Which of the following graph can be used to detect seasonality in time series data?,D,,Multiple box,Autocorrelation,None of the above,All of the above,,,,Seasonality is a presence of variations at specific periodic intervals. The variation of distribution can be observed in multiple box plots. And thus seasonality can be easily spotted. Autocorrelation plot should show spikes at lags equal to the period.
17,Stationarity is a desirable property for analyzing time series.,A,,TRUE,FALSE,,,,,,"When the following conditions are satisfied then a time series is stationary.

1.Mean is constant and does not depend on time
2.Autocovariance function depends on s and t only through their difference |s-t| (where t and s are moments in time)
3.The time series under considerations is a finite variance process

These conditions are essential prerequisites for mathematically representing a time series to be used for analysis and forecasting. Thus stationarity is a desirable property."
18,Which of the following is incorrect?,A,,"AIC value the bigger, the better",We want to see the p-value of the Ljung-Box Pierce test close to 0,Today's price = yesterday's price + a change that is independent of all previous information can be considered as a Random Walk Model.,None of the above,,,,"Based on the formula of AIC: AIC = -2*ln(likelihood) + 2*k, the less the AIC value is, the more fit the model with the time series. "
19,"Imagine, you are working on a time series dataset. Your manager has asked you to build a highly accurate model. You started to build two types of models which are given below.

Model 1: Decision Tree model

Model 2: Time series regression model

At the end of evaluation of these two models, you found that model 2 is better than model 1. What could be the possible reason for your inference?",A,,Model 1 couldn't map the linear relationship as good as Model 2,Model 1 will always be better than Model 2,You can't compare decision tree with time series regression,None of the above,,,,A time series model is similar to a regression model. So it is good at finding simple linear relationships. While a tree based model though efficient will not be as good at finding and exploiting linear relationships.
20,What type of analysis could be most effective for predicting temperature on the following type of data.,A,,Time Series Analysis,Classification,Clustering,None of the above,,20.PNG,,The data is obtained on consecutive days and thus the most effective type of analysis will be time series analysis.
21,What is the first difference of temperature / precipitation variable?,B,,"9-7 = 2, 9.2-9 = 0.2, 10-9.2 = 0.8, 12-10 = 2, 11-12 = -1","73.17-35 = 38.17, 27.06-73.17 = -46.11, 22.08 - 27.06 = -4.98, 36.36 - 22.08 = 14.29, 13.75 - 36.36 = -22.61","35, 73.17-35 = 38.17, 27.06-73.17 = -46.11, 22.08 - 27.06 = -4.98, 36.36 - 22.08 = 14.29,13.75 - 36.3 = -22.61",,,21.PNG,,"$$73.17-35 = 38.17$$
$$27.05-73.17 = -46.11 and so on..$$
$$13.75 - 36.36 = -22.61$$"
22,Any stationary time series can be approximately the random superposition of sines and cosines oscillating at various frequencies.,A,,TRUE,FALSE,,,,,,"A weakly stationary time series, \(x_t\), is a finite variance process such that:
The mean value function,  \(\mu t\) , is constant and does not depend on time t, and (ii) the autocovariance function,  \(\gamma (s,t)\) , defined in depends on s and t only through their difference |s-t|.
Random superposition of sines and cosines oscillating at various frequencies is white noise. white noise is weakly stationary or stationary. If the white noise variates are also normally distributed or Gaussian, the series is also strictly stationary."
23,Autocovariance function for weakly stationary time series does not depend on _______ ?,C,,Separation of xs and xt,h = | s - t |,Location of point at a particular time,,,,,"By definition of weak stationary time series: A weakly stationary time series, \(x_t\, is a finite variance process such that:
The mean value function,  \(\mu t\) , is constant and does not depend on time t, and (ii) the autocovariance function,  \(\gamma (s,t)\) , defined in depends on s and t only through their difference |s-t|."
24,Two time series are jointly stationary if _____ ?,D,,They are each stationary,Cross variance function is a function only of lag h,Only A,Both A and B,,,,Joint stationarity is defined based on the above two mentioned conditions.
25,In autoregressive models _______ ?,C,,Current value of dependent variable is influenced by current values of independent variables,Current value of dependent variable is influenced by current and past values of independent variables,Current value of dependent variable is influenced by past values of both dependent and independent variables,None of the above,,,,"Autoregressive models are based on the idea that the current value of the series, \(x_t\), can be explained as a function of p past values, \(x_{t-1},x_{t-2},...,x_{t-p}\), where p determines the number of steps into the past needed to forecast the current value. Ex. \(x_t = x_{t-1} - 0.90x_t-2 + w_t\), 

Where \(x_{t-1}\)  and \(x_{t-2}\) are past values of dependent variable and wt the white noise can represent values of independent values. 
The example can be extended to include multiple series analogous to multivariate linear regression."
26,For MA (Moving Average) models the pair \(\sigma = 1\) and \(\theta = 5\) yields the same autocovariance function as the pair \(\sigma = 25\) and \(\theta = 1/5\).,A,,TRUE,FALSE,,,,27.PNG,,"True, because autocovariance is invertible for MA models
note that for an MA(1) model,  \(\rho (h)\) is the same for  \(\theta\)  and \(1 / \theta\) 
try 5 and 1/5, for example. In addition, the pair  \(\sigma^2 w = 1\)  and  \(\theta = 5\)  yield the same autocovariance function as the pair   \(\sigma^2 w = 25\)  and \(\theta = 1/5\) ."
27,How many AR and MA terms should be included for the time series by looking at the above ACF and PACF plots?,B,,AR(1) MA(0),AR(0)MA(1),AR(2)MA(1),AR(1)MA(2),Can't say,28.PNG,,Strong negative correlation at lag 1 suggest MA and there is only 1 significant lag.
28,Which of the following is true for white noise?,C,,Mean =0,Zero autocovariances,Zero autocovariances except at lag zero,Quadratic Variance,,,,"A white noise process must have a constant mean, a constant variance and no autocovariance structure (except at lag zero, which is the variance)."
29,"For the following MA (3) process  \(y_t = \mu + E_t + {\theta}_1 E_{t-1} + {\theta}_2 E_{t-2} +{\theta}_3 E_{t-3}\)  , where  \(\sigma_t\)  is a zero mean white noise process with variance  \(\sigma^2\)",B,,ACF = 0 at lag 3,ACF =0 at lag 5,ACF =1 at lag 1,ACF =0 at lag 2,ACF = 0 at lag 3 and at lag 5,,,"Recall that an MA(q) process only has memory of length q. This means that all of the autocorrelation coefficients will have a value of zero beyond lag q. This can be seen by examining the MA equation, and seeing that only the past q disturbance terms enter into the equation, so that if we iterate this equation forward through time by more than q periods, the current value of the disturbance term will no longer affect y. Finally, since the autocorrelation function at lag zero is the correlation of y at time t with y at time t (i.e. the correlation of \(y_t\) with itself), it must be one by definition."
30,"Consider the following AR(1) model with the disturbances having zero mean and unit variance.
The (unconditional) variance of y will be given by ?",B,,1/(0.2^2)= 1/0.04 = 25,1/(1-(0.2^2))= 1/0.96 = 1.04,(1-(0.2^2))= 0.96,1/0.2 = 5,,31.PNG,,Variance of the disturbances divided by (1 minus the square of the autoregressive coefficient). Which in this case is : 1/(1-(0.2^2))= 1/0.96= 1.041
31,The pacf (partial autocorrelation function) is helpful for distinguishing between ______ ?,B,,An AR and MA model,An AR and an ARMA,An MA and an ARMA,Different models from within the ARMA family,,,,"AR(p): ACF tails off, PACF cuts off after lag p. 
MA(q): ACF cuts off after lag p, PACF tails off. 
ARMA(p,q): Both ACF & PACF tails off."
32,Second differencing in time series can help to eliminate which trend?,A,,Quadratic Trend,Linear Trend,Both A & B,None of the above,,,,"The first difference is denoted as  \(\Delta x_t = x_t - x_{t-1}\) . (1) 
As we have seen, the first difference eliminates a linear trend. A second difference, that is, the difference of (1), can eliminate a quadratic trend, and so on."
33,Which of the following cross validation techniques is better suited for time series data?,D,,k-Fold Cross Validation,Leave-one-out Cross Validation,Stratified Shuffle Split Cross Validation,Forward Chaining Cross Validation,,,,"Time series is ordered data. So the validation data must be ordered to. Forward chaining ensures this. It works as follows:

fold 1 : training [1], test [2]
fold 2 : training [1 2], test [3]
fold 3 : training [1 2 3], test [4]
fold 4 : training [1 2 3 4], test [5]
fold 5 : training [1 2 3 4 5], test [6]"
34,BIC penalizes complex models more strongly than the AIC.,A,,TRUE,FALSE,,,,,,"$$AIC = -2*ln(likelihood) + 2*k, BIC = -2*ln(likelihood) + ln(N)*k$$
where: k = model degrees of freedom, N = number of observations.     
At relatively low N (7 and less) BIC is more tolerant of free parameters than AIC, but less tolerant at higher N (as the natural log of N overcomes 2)."
35,"The figure below shows the estimated autocorrelation and partial autocorrelations of a time series of n = 60 observations. Based on these plots, we should.",B,,Transform the data by taking logs,Difference the series to obtain stationary data,Fit an MA(1) model to the time series,,,36.PNG,,"The autocorr shows a definite trend and partial autocorrelation shows a choppy trend, in such a scenario taking a log would be of no use. Differencing the series to obtain a stationary series is the only option."
36,"Use the estimated exponential smoothening given above and predict temperature for the next 3 years (1998-2000)

These results summarize the fit of a simple exponential smooth to the time series.",B,,"0.35,0.35,0.35 caluclated by: 0.3968*0.43 + (1 - 0.3968)*0.2924= 0.3470","0.33,0.33,0.33 caluclated by: 0.3968*0.43 + (1 - 0.3968)*0.2637= 0.3297","0.28,0.28,0.28 caluclated by: 0.3968*0.43 + 0.3968*0.2637= 0.2753",No correct answer,,37.PNG,,"The predicted value from the exponential smooth is the same for all 3 years, so all we need is the value for next year. The expression for the smooth is
 $$smooth_t = \alpha y_{t-1} + (1- \alpha) smooth_{t-1} $$
Hence, for the next point, the next value of the smooth (the prediction for the next observation) is
is
$$smooth_n = \alpha y_{n-1} + (1- \alpha) smooth_{n-1}= 0.3968*0.43 + (1 - 0.3968)* 0.2637= 0.3297 $$"
37,"Find 95% prediction intervals for the predictions of temperature in 1999. 

These results summarize the fit of a simple exponential smooth to the time series.",B,,0.32972 * 0.1125,\(0.32972 * 0.1125 * sqrt{1+ 0.3968} \approx 0.32972 * 0.121\),0.32972 * 0.129,No correct answer,,38.PNG,,"The sd of the prediction errors is $$1 period out 0.1125$$
$$2 periods out  \(0.1125 * sqrt{1+ \alpha 2} = 0.1125 * sqrt{1+ 0.3968} \approx 0.121 \)$$"
38,Which of the following statement is correct?,B,,"If autoregressive parameter (p) in an ARIMA model is 1, it means that there is no auto-correlation in the series.","If moving average component (q) in an ARIMA model is 1, it means that there is auto-correlation in the series with lag 1.","If integrated component (d) in an ARIMA model is 0, it means that the series is not stationary.",All of the above,None of the above,,,"Autoregressive component: AR stands for autoregressive.  Autoregressive parameter is denoted by p.  When p =0, it means that there is no auto-correlation in the series.  When p=1, it means that the series auto-correlation is till one lag. 
Integrated: In ARIMA time series analysis, integrated is denoted by d.  Integration is the inverse of differencing.  When d=0, it means the series is stationary and we do not need to take the difference of it.  When d=1, it means that the series is not stationary and to make it stationary, we need to take the first difference.  When d=2, it means that the series has been differenced twice.  Usually, more than two time difference is not reliable. 
Moving average component: MA stands for moving the average, which is denoted by q.  In ARIMA, moving average q=1 means that it is an error term and there is auto-correlation with one lag."
39,"In a time-series forecasting problem, if the seasonal indices for quarters 1, 2, and 3 are 0.80, 0.90, and 0.95 respectively. What can you say about the seasonal index of quarter 4?",B,,It will be less than 1,It will be greater than 1,It will be equal to 1,Seasonality does not exist,Data is insufficient,,,"The seasonal indices must sum to 4, since there are 4 quarters. .80 + .90 + .95 = 2.65, so the seasonal index for the 4th quarter must be 1.35 so B is the correct answer."
